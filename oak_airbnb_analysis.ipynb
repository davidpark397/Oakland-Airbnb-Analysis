{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oakland Airbnb Listing Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that I will be using for this analysis consists of Airbnb listings characteristics for the city of Oakland, CA from http://insideairbnb.com/get-the-data.html compiled on July 8, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyses produced will mostly be for the purpose of understanding the drivers of an Oakland Airbnb listing's price. We will start off by analyzing and visualizing basic trends in the data. Once we have gained a general idea of how price is affected across numerous different factors, we will apply regularized linear models in order to predict price itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>access</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_response_time</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>neighbourhood_cleansed</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>amenities</th>\n",
       "      <th>price</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>cancellation_policy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3083</td>\n",
       "      <td>This is a beautiful spacious Guest House with ...</td>\n",
       "      <td>Guest have access to kitchen and shared bathro...</td>\n",
       "      <td>3518</td>\n",
       "      <td>within a few hours</td>\n",
       "      <td>90%</td>\n",
       "      <td>f</td>\n",
       "      <td>Prescott</td>\n",
       "      <td>37.80832</td>\n",
       "      <td>-122.29300</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{TV,\"Cable TV\",Internet,Wifi,Kitchen,\"Free str...</td>\n",
       "      <td>$65.00</td>\n",
       "      <td>167</td>\n",
       "      <td>34</td>\n",
       "      <td>90.0</td>\n",
       "      <td>f</td>\n",
       "      <td>strict_14_with_grace_period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5739</td>\n",
       "      <td>The Mod Studio is a cozy, clean and convenient...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9276</td>\n",
       "      <td>within an hour</td>\n",
       "      <td>100%</td>\n",
       "      <td>t</td>\n",
       "      <td>Oakland Ave-Harrison St</td>\n",
       "      <td>37.81503</td>\n",
       "      <td>-122.26212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{TV,\"Cable TV\",Internet,Wifi,Kitchen,Cat(s),\"F...</td>\n",
       "      <td>$100.00</td>\n",
       "      <td>12</td>\n",
       "      <td>227</td>\n",
       "      <td>99.0</td>\n",
       "      <td>f</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11022</td>\n",
       "      <td>The Rockridge Bungalow is the lower unit of a ...</td>\n",
       "      <td>Entire fully-equipped 1BR apartment approximat...</td>\n",
       "      <td>40142</td>\n",
       "      <td>within an hour</td>\n",
       "      <td>100%</td>\n",
       "      <td>t</td>\n",
       "      <td>Shafter</td>\n",
       "      <td>37.83771</td>\n",
       "      <td>-122.25191</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{TV,\"Cable TV\",Internet,Wifi,Kitchen,\"Free str...</td>\n",
       "      <td>$141.00</td>\n",
       "      <td>260</td>\n",
       "      <td>120</td>\n",
       "      <td>95.0</td>\n",
       "      <td>t</td>\n",
       "      <td>strict_14_with_grace_period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13291</td>\n",
       "      <td>Relax in a darling saltbox-style home in our u...</td>\n",
       "      <td>Full kitchen, food storage in pantry, laundry ...</td>\n",
       "      <td>51853</td>\n",
       "      <td>within a few hours</td>\n",
       "      <td>100%</td>\n",
       "      <td>f</td>\n",
       "      <td>Santa Fe</td>\n",
       "      <td>37.84374</td>\n",
       "      <td>-122.27583</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{Internet,Wifi,Kitchen,\"Pets live on this prop...</td>\n",
       "      <td>$70.00</td>\n",
       "      <td>259</td>\n",
       "      <td>58</td>\n",
       "      <td>94.0</td>\n",
       "      <td>f</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24916</td>\n",
       "      <td>Studio with patio garden just out the door.  E...</td>\n",
       "      <td>Patio, garden, washer,dryer, shared</td>\n",
       "      <td>98716</td>\n",
       "      <td>within a few hours</td>\n",
       "      <td>90%</td>\n",
       "      <td>f</td>\n",
       "      <td>Piedmont Avenue</td>\n",
       "      <td>37.83326</td>\n",
       "      <td>-122.25033</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>{TV,Internet,Wifi,Kitchen,\"Free street parking...</td>\n",
       "      <td>$91.00</td>\n",
       "      <td>295</td>\n",
       "      <td>103</td>\n",
       "      <td>90.0</td>\n",
       "      <td>t</td>\n",
       "      <td>strict_14_with_grace_period</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        description  \\\n",
       "0   3083  This is a beautiful spacious Guest House with ...   \n",
       "1   5739  The Mod Studio is a cozy, clean and convenient...   \n",
       "2  11022  The Rockridge Bungalow is the lower unit of a ...   \n",
       "3  13291  Relax in a darling saltbox-style home in our u...   \n",
       "4  24916  Studio with patio garden just out the door.  E...   \n",
       "\n",
       "                                              access  host_id  \\\n",
       "0  Guest have access to kitchen and shared bathro...     3518   \n",
       "1                                                NaN     9276   \n",
       "2  Entire fully-equipped 1BR apartment approximat...    40142   \n",
       "3  Full kitchen, food storage in pantry, laundry ...    51853   \n",
       "4                Patio, garden, washer,dryer, shared    98716   \n",
       "\n",
       "   host_response_time host_response_rate host_is_superhost  \\\n",
       "0  within a few hours                90%                 f   \n",
       "1      within an hour               100%                 t   \n",
       "2      within an hour               100%                 t   \n",
       "3  within a few hours               100%                 f   \n",
       "4  within a few hours                90%                 f   \n",
       "\n",
       "    neighbourhood_cleansed  latitude  longitude             ...               \\\n",
       "0                 Prescott  37.80832 -122.29300             ...                \n",
       "1  Oakland Ave-Harrison St  37.81503 -122.26212             ...                \n",
       "2                  Shafter  37.83771 -122.25191             ...                \n",
       "3                 Santa Fe  37.84374 -122.27583             ...                \n",
       "4          Piedmont Avenue  37.83326 -122.25033             ...                \n",
       "\n",
       "  bathrooms bedrooms beds                                          amenities  \\\n",
       "0       1.0      1.0  1.0  {TV,\"Cable TV\",Internet,Wifi,Kitchen,\"Free str...   \n",
       "1       1.0      1.0  1.0  {TV,\"Cable TV\",Internet,Wifi,Kitchen,Cat(s),\"F...   \n",
       "2       1.0      1.0  1.0  {TV,\"Cable TV\",Internet,Wifi,Kitchen,\"Free str...   \n",
       "3       1.0      1.0  1.0  {Internet,Wifi,Kitchen,\"Pets live on this prop...   \n",
       "4       1.0      0.0  2.0  {TV,Internet,Wifi,Kitchen,\"Free street parking...   \n",
       "\n",
       "     price  availability_365  number_of_reviews review_scores_rating  \\\n",
       "0   $65.00               167                 34                 90.0   \n",
       "1  $100.00                12                227                 99.0   \n",
       "2  $141.00               260                120                 95.0   \n",
       "3   $70.00               259                 58                 94.0   \n",
       "4   $91.00               295                103                 90.0   \n",
       "\n",
       "  instant_bookable          cancellation_policy  \n",
       "0                f  strict_14_with_grace_period  \n",
       "1                f                     moderate  \n",
       "2                t  strict_14_with_grace_period  \n",
       "3                f                     moderate  \n",
       "4                t  strict_14_with_grace_period  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Read the file and take an initial look at the data\n",
    "listings_data = pd.read_csv('listings_full.csv')\n",
    "\n",
    "#Take a look at the data\n",
    "# listings_data\n",
    "\n",
    "# Import only the columns that we deem important/usable\n",
    "cols = ['id', \n",
    "        'description', \n",
    "        'access', \n",
    "        'host_id',\n",
    "        'host_response_time', \n",
    "        'host_response_rate', \n",
    "        'host_is_superhost', \n",
    "        'neighbourhood_cleansed',\n",
    "        'latitude', \n",
    "        'longitude', \n",
    "        'is_location_exact',\n",
    "        'property_type', \n",
    "        'room_type', \n",
    "        'accommodates', \n",
    "        'bathrooms', \n",
    "        'bedrooms', \n",
    "        'beds', \n",
    "        'amenities', \n",
    "        'price', \n",
    "        'availability_365', \n",
    "        'number_of_reviews', \n",
    "        'review_scores_rating', \n",
    "        'instant_bookable', \n",
    "        'cancellation_policy']\n",
    "\n",
    "listings_data = listings_data[cols]\n",
    "listings_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3211, 24)\n"
     ]
    }
   ],
   "source": [
    "print(listings_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have around 3200 airbnb listings for the city of Oakland in the dataset. We have 24 features that we've extracted for each of those listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other basic questions that can be answered very quickly: How many unique hosts are there? How many listings does the average host have? What is the maximum number of listings a host is associated with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that no single host is associated with multiple host ID's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Hosts: 2288\n",
      "Maximum number of listings a host has: 34\n",
      "Average number of listings per host: 1.4034090909090908\n"
     ]
    }
   ],
   "source": [
    "print('Number of Unique Hosts: {}'.format(len(listings_data.groupby('host_id').count())))\n",
    "print('Maximum number of listings a host has: {}'.format(listings_data.groupby('host_id').size().max()))\n",
    "print('Average number of listings per host: {}'.format(listings_data.groupby('host_id').size().mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see if we do indeed have missing data, how much missing data, and for what column/specification. We should drop the listings that don't have information that we absolutely need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "description                 32\n",
       "access                    1010\n",
       "host_id                      0\n",
       "host_response_time         866\n",
       "host_response_rate         866\n",
       "host_is_superhost            0\n",
       "neighbourhood_cleansed       0\n",
       "latitude                     0\n",
       "longitude                    0\n",
       "is_location_exact            0\n",
       "property_type                0\n",
       "room_type                    0\n",
       "accommodates                 0\n",
       "bathrooms                    1\n",
       "bedrooms                     1\n",
       "beds                         2\n",
       "amenities                    0\n",
       "price                        0\n",
       "availability_365             0\n",
       "number_of_reviews            0\n",
       "review_scores_rating       540\n",
       "instant_bookable             0\n",
       "cancellation_policy          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have the most instances of missing data for the `access` column which contains directions with regards to getting into the listing (where the host has put the key, how the lock works, etc). In my experience \n",
    "\n",
    "We also have a substantial amount of missing data in the `host_response_time` and `host_response_rate` columns which contain information on how quickly a host responds to a request and how often a host responds to a request (expressed as a percentage) respectively. This is probably due to a number of different factors. \n",
    "\n",
    "And finally, we also have a substantial amount of missing data in the `review_scores_rating` column, which essentially is a score/100 representing the guest's satisfaction with the listing. \n",
    "\n",
    "We essentially want to handle these missing points of data in way so that we avoid running into errors that may occur by having missing values when we use certain operations on columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type Consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the data types of each column may be informative when it comes to the decision that we should make in handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          int64\n",
       "description                object\n",
       "access                     object\n",
       "host_id                     int64\n",
       "host_response_time         object\n",
       "host_response_rate         object\n",
       "host_is_superhost          object\n",
       "neighbourhood_cleansed     object\n",
       "latitude                  float64\n",
       "longitude                 float64\n",
       "is_location_exact          object\n",
       "property_type              object\n",
       "room_type                  object\n",
       "accommodates                int64\n",
       "bathrooms                 float64\n",
       "bedrooms                  float64\n",
       "beds                      float64\n",
       "amenities                  object\n",
       "price                      object\n",
       "availability_365            int64\n",
       "number_of_reviews           int64\n",
       "review_scores_rating      float64\n",
       "instant_bookable           object\n",
       "cancellation_policy        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `host_response_time` and `host_response_rate` are objects. Let's take a closer look at both, starting with `host_response_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within an hour        1530\n",
      "within a few hours     489\n",
      "within a day           287\n",
      "a few days or more      39\n",
      "Name: host_response_time, dtype: int64\n",
      "\n",
      "Data Type of element: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(listings_data['host_response_time'].value_counts())\n",
    "print('\\nData Type of element: {}'.format(type(listings_data['host_response_time'].iloc[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `host_response_time` column consists of string values. Let's replace the missing values with the string 'missing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_data['host_response_time'].fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the `host_response_rate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%    1871\n",
      "90%       70\n",
      "97%       50\n",
      "95%       42\n",
      "80%       40\n",
      "98%       23\n",
      "89%       22\n",
      "50%       21\n",
      "67%       20\n",
      "0%        19\n",
      "99%       18\n",
      "70%       14\n",
      "60%       14\n",
      "75%       14\n",
      "92%       12\n",
      "Name: host_response_rate, dtype: int64\n",
      "\n",
      "Data Type of element: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(listings_data['host_response_rate'].value_counts().head(15))\n",
    "print('\\nData Type of element: {}'.format(type(listings_data['host_response_rate'].iloc[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first fill the missing values with strings so that we don't run into errors while attempting to change the entire column. Then, let's change the strings into the corresponding integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with a placeholder string\n",
    "listings_data['host_response_rate'].fillna('999%', inplace=True)\n",
    "\n",
    "# Remove percentage sign from the string and make int\n",
    "listings_data['host_response_rate'] = listings_data['host_response_rate'].apply(lambda x:int(x[:len(x)-1]));\n",
    "\n",
    "# Replace placeholder int with NaN, calculate mean and fill NaN with mean\n",
    "host_response_rate_mean = listings_data['host_response_rate'].mean()\n",
    "listings_data['host_response_rate'].replace(999, np.nan, inplace=True)\n",
    "listings_data['host_response_rate'].fillna(host_response_rate_mean, inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to handle the `review_scores_rating` column in a similar way. We can go ahead and do this without manipulation because we know that the type of the column is `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_scores_mean = listings_data['review_scores_rating'].mean()\n",
    "listings_data['review_scores_rating'].fillna(review_scores_mean, inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the description and access columns, which we know contain strings as entries, let's fill the missing values with the 'missing' string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_data['description'].fillna('missing', inplace=True)\n",
    "listings_data['access'].fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's change the price column from a string to a corresponding integer for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that removes commas, and converts the string into a float\n",
    "def price_float(price):\n",
    "    price = price.replace(',', '')\n",
    "    return float(price[1:]) # Remove the $ sign\n",
    "\n",
    "# Apply it\n",
    "listings_data['price'] = listings_data['price'].apply(price_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our work here is not done. We will need to further prepare the data for modeling (fixing skew through transformation, creating categorical variables, etc) later. However, these basic changes should let us explore the data with relative ease. Let's save this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_data.to_csv('listings_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations and Basic Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first plot all of the listings on a map so that we can see their locations geographically. It may be helpful to separate the listings by room_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entire home/apt    1926\n",
       "Private room       1109\n",
       "Shared room         176\n",
       "Name: room_type, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "room_types = listings_data['room_type'].value_counts()\n",
    "room_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the listings by room type\n",
    "homes = listings_data[listings_data['room_type'] == 'Entire home/apt']\n",
    "priv_rooms = listings_data[listings_data['room_type'] == 'Private room']\n",
    "shar_rooms = listings_data[listings_data['room_type'] == 'Shared room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Initialize Basemap with longitude/latitude extents of the city of Oakland\n",
    "ll = [37.715394, -122.351381]\n",
    "ur = [37.878177, -122.108927]\n",
    "\n",
    "oakmap = Basemap(llcrnrlon=ll[1], \n",
    "                 llcrnrlat=ll[0],\n",
    "                 urcrnrlon=ur[1], \n",
    "                 urcrnrlat=ur[0],\n",
    "                 projection='gall',\n",
    "                 resolution='f', epsg=4269)\n",
    "\n",
    "# Draw the city\n",
    "oakmap.drawcoastlines()\n",
    "oakmap.drawcountries()\n",
    "oakmap.fillcontinents(color = 'w')\n",
    "oakmap.drawmapboundary(fill_color='dodgerblue')\n",
    "\n",
    "# Extract the latitude and longitudes of the room types\n",
    "home_long, home_lat = oakmap(homes['longitude'], homes['latitude'])\n",
    "proom_long, proom_lat = oakmap(priv_rooms['longitude'], priv_rooms['latitude'])\n",
    "sroom_long, sroom_lat = oakmap(shar_rooms['longitude'], shar_rooms['latitude'])\n",
    "\n",
    "# Pre-determined latitude and longitudes of Oakland Bart stations\n",
    "bart_lat = [37.759186, \n",
    "            37.848407, \n",
    "            37.830242, \n",
    "            37.797313,\n",
    "            37.804508,\n",
    "            37.802175,\n",
    "            37.803869,\n",
    "            37.806992]\n",
    "\n",
    "bart_long = [-122.197585,\n",
    "             -122.251871,\n",
    "             -122.267203,\n",
    "             -122.265287,\n",
    "             -122.295312,\n",
    "             -122.271978,\n",
    "             -122.271617,\n",
    "             -122.269513]\n",
    "\n",
    "blo, bla = oakmap(bart_long, bart_lat)\n",
    "\n",
    "# Plot the room types by room type, plot the bar stations on the map\n",
    "oakmap.plot(home_long, home_lat, 'o', color='seagreen', markersize=4)\n",
    "oakmap.plot(proom_long, proom_lat, 'o', color='tomato', markersize=4)\n",
    "oakmap.plot(sroom_long, sroom_lat, 'o', color='royalblue', markersize=4)\n",
    "oakmap.plot(blo, bla, '*', color='lightskyblue', markeredgecolor='k', markersize=23)\n",
    "\n",
    "# Create legend\n",
    "Home = mpatches.Patch(color='seagreen', label='Home')\n",
    "PRoom = mpatches.Patch(color='tomato', label='Private Room')\n",
    "SRoom = mpatches.Patch(color='royalblue', label='Single Room')\n",
    "plt.legend(handles=[Home, PRoom, SRoom], loc=1)\n",
    "\n",
    "plt.savefig('Figures\\\\oak_basemap_proptype.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a general idea of the locations of the listings, the room type makeup of the listings, as well as the locations of the bart stations! It might be an interesting exercise to do the same plot with the listings divided into groups that reflect price. One thing to point out is that the listings seem to be more concentrated toward the more \"city-like\" neighbhorhoods within Oakland and are more sparse towards the southeast area of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the listings by price\n",
    "print(listings_data['price'].describe())\n",
    "\n",
    "# Obtained quartile cutoffs\n",
    "low = 63\n",
    "medium = 95\n",
    "high = 150\n",
    "\n",
    "# Bucket the listings based on the prices\n",
    "cheap = listings_data[listings_data['price'] < low]\n",
    "cnormal = listings_data[(listings_data['price'] > low) & (listings_data['price'] < medium)]\n",
    "enormal = listings_data[(listings_data['price'] > medium) & (listings_data['price'] < high)]\n",
    "expensive = listings_data[listings_data['price'] > high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Initialize Basemap with longitude/latitude extents of the city of Oakland\n",
    "ll = [37.715394, -122.351381]\n",
    "ur = [37.878177, -122.108927]\n",
    "\n",
    "oakmap = Basemap(llcrnrlon=ll[1], \n",
    "                 llcrnrlat=ll[0],\n",
    "                 urcrnrlon=ur[1], \n",
    "                 urcrnrlat=ur[0],\n",
    "                 projection='gall',\n",
    "                 resolution='f', epsg=4269)\n",
    "\n",
    "# Draw the city\n",
    "oakmap.drawcoastlines()\n",
    "oakmap.drawcountries()\n",
    "oakmap.fillcontinents(color = 'w')\n",
    "oakmap.drawmapboundary(fill_color='dodgerblue')\n",
    "\n",
    "# Extract the latitude and longitudes of the listings associated with different prices\n",
    "cheap_long, cheap_lat = oakmap(cheap['longitude'], cheap['latitude'])\n",
    "cnormal_long, cnormal_lat = oakmap(cnormal['longitude'], cnormal['latitude'])\n",
    "enormal_long, enormal_lat = oakmap(enormal['longitude'], enormal['latitude'])\n",
    "expen_long, expen_lat = oakmap(expensive['longitude'], expensive['latitude'])\n",
    "\n",
    "# Pre-determined latitude and longitudes of Oakland Bart stations\n",
    "bart_lat = [37.759186, \n",
    "            37.848407, \n",
    "            37.830242, \n",
    "            37.797313,\n",
    "            37.804508,\n",
    "            37.802175,\n",
    "            37.803869,\n",
    "            37.806992]\n",
    "\n",
    "bart_long = [-122.197585,\n",
    "             -122.251871,\n",
    "             -122.267203,\n",
    "             -122.265287,\n",
    "             -122.295312,\n",
    "             -122.271978,\n",
    "             -122.271617,\n",
    "             -122.269513]\n",
    "\n",
    "blo, bla = oakmap(bart_long, bart_lat)\n",
    "\n",
    "# Plot the rooms by price quartile, plot the bart station\n",
    "oakmap.plot(cheap_long, cheap_lat, 'o', color='seagreen', markersize=4)\n",
    "oakmap.plot(cnormal_long, cnormal_lat, 'o', color='tomato', markersize=4)\n",
    "oakmap.plot(enormal_long, enormal_lat, 'o', color='darkkhaki', markersize=4)\n",
    "oakmap.plot(expen_long, expen_lat, 'o', color='royalblue', markersize=4)\n",
    "oakmap.plot(blo, bla, '*', color='lightskyblue', markeredgecolor='k', markersize=23)\n",
    "\n",
    "# Create legend\n",
    "Cheap = mpatches.Patch(color='seagreen', label='1st Price Quartile')\n",
    "CNormal = mpatches.Patch(color='tomato', label='2nd Price Quartile')\n",
    "ENormal = mpatches.Patch(color='darkkhaki', label='3rd Price Quartile')\n",
    "Expen = mpatches.Patch(color='royalblue', label='4th Price Quartile')\n",
    "plt.legend(handles=[Cheap, CNormal, ENormal, Expen], loc=1)\n",
    "\n",
    "plt.savefig('Figures\\\\oak_basemap_priceQ.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was expecting to see some visibily apparent price-location relationship here, maybe by neighborhood or by proximity to a BART station, but there does not seem to be anything of the sort. Let's create some more visualizations - next I'd like to examine the number of listings by neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_data.groupby('neighbourhood_cleansed').count().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 124 neighborhoods in total!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the 30 neighborhoods with the most listings\n",
    "neighborhood_freq = listings_data['neighbourhood_cleansed'].value_counts().head(30)\n",
    "\n",
    "# Plot bar graph\n",
    "neighborhood_freq.plot(kind='bar', \n",
    "                       figsize=(15, 6), \n",
    "                       color='royalblue')\n",
    "\n",
    "# Change plot specifications\n",
    "plt.title('Neighborhood Frequency (Top 30)')\n",
    "plt.rc('font', size=10)\n",
    "plt.xlabel('Neighborhood')\n",
    "plt.ylabel('Number of Listings')\n",
    "\n",
    "plt.savefig('Figures\\\\neighborhood_freq.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above gives us a general idea of the most popular neighborhoods in Oakland in terms of the number of Airbnb listings contained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at listing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_data['price'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also may be a useful exercise to examine the mean price per neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the 30 most expensive neighbhoords\n",
    "mean_neighborhood_prices = listings_data.groupby('neighbourhood_cleansed')['price'].mean().sort_values(ascending=False)[listings_data['neighbourhood_cleansed'].value_counts() > 55]\n",
    "\n",
    "# Plot bar graph\n",
    "mean_neighborhood_prices.plot(kind='bar', \n",
    "                              figsize=(15, 6), \n",
    "                              color='seagreen')\n",
    "\n",
    "# Change plot specifications\n",
    "plt.title('Mean Listing Price by Neighborhood')\n",
    "plt.rc('font', size=15)\n",
    "plt.xlabel('Neighborhood')\n",
    "plt.ylabel('Average Listing Price')\n",
    "\n",
    "plt.savefig('Figures\\\\neighborhood_price.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I've only elected to calculate the mean listings price by neighorhood for neighborhoods that have more than the average number of listings per neighborhood. This would make the metric that we want to examine in order to characterize more robust to the presence of outliers. However, it's important to keep in mind that this still may be a bit misleading. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UR = listings_data[listings_data['neighbourhood_cleansed'] == 'Upper Rockridge']\n",
    "print('Shape: {}'.format(UR.shape))\n",
    "print('Maximum Price in Upper Rockridge Neighborhood: {}'.format(UR['price'].max()))\n",
    "print('Median Price in Upper Rockridge Neighborhood: {}'.format(UR['price'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of listings in the Upper Rockridge neighborhood (supposedly the most expensive neighborhood) is 61. Given that there are so few listings in the area compared to Bushrod, for example, the average listings price of the neighborhood is more easily skewed by the presence of outliers. We can see that this is may be occurring here, as the maximum listing price in the neighborhood (\\\\$2500.00) is multiple times more than the median listing price in the neighborhood (\\\\$149.00). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be more useful, then, to simply view a DataFrame with the Neighborhood, average price, and the number of listings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the mean price of the listings in each neighborhood and packages as dataframe\n",
    "nh_df = pd.DataFrame(listings_data.groupby('neighbourhood_cleansed')['price'].mean())\n",
    "\n",
    "# Create a column containing the number of listings in each neighborhood\n",
    "nh_df['Number of listings'] = listings_data.groupby('neighbourhood_cleansed').count()['id']\n",
    "nh_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further analysis is needed in order to characterize \"expensive\" neighborhoods in Oakland. We have more metadata here to examine in order to get an idea of whether or not listings prices by neighborhood are driven by the neighborhoods themselves or other factors. Specifically, let's keep an eye out for characteristics such as the room type, property type, # of people the listing can accommodate, # of beds/bathrooms, etc. and how they affect listing price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's take a look at the property and room type distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_types = listings_data['property_type'].value_counts()\n",
    "property_types.plot(kind='bar', \n",
    "                    figsize=(12, 6), \n",
    "                    color='mediumturquoise')\n",
    "\n",
    "plt.title('Property Type Frequency')\n",
    "plt.rc('font', size=15)\n",
    "plt.xlabel('Property Type')\n",
    "plt.ylabel('Number of Listings')\n",
    "\n",
    "plt.savefig('Figures\\\\prop_type_freq.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's apparent that the most common property types in descending order, are houses, apartments, guest suites, guesthouses and condominiums. There are also some other property types that would be less familiar to AirBnB users including \"bungalows\", \"campers\", and \"dome houses\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regards to room types, we already know that there are only 3 unique types (from the Basemap visualization). Because the Basemap visualization doesn't really give us a sense of the distribution of listings between those three unique room types, a small pie graph may be suitable for visualization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_types.plot(kind='pie', \n",
    "                figsize=(8, 8), \n",
    "                autopct = '%.2f')\n",
    "\n",
    "plt.title('Distribution of Room Types')\n",
    "plt.rc('font', size=12)\n",
    "\n",
    "plt.savefig('Figures\\\\room_type_distribution.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a majority (around 60%) of listings offer the entire residence. This majority is followed by around 34.5% of listings which offer private rooms, and the remaining 5.5% of listings that offer a shared room. Now that we have an idea of the distribution of listings across room types and property types, we should look at how mean prices vary across room type and property type combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the room type, property type, and price columns\n",
    "room_prop = listings_data[['room_type', 'property_type', 'price']]\n",
    "\n",
    "# Group by room type and property type, then find mean price\n",
    "mean_room_prop_prices = room_prop.groupby(['room_type', 'property_type']).mean()\n",
    "\n",
    "# Essentially make into a pivot table, then fill nonexistent combinations with \"NaC\"\n",
    "mean_room_prop_prices = mean_room_prop_prices.unstack().transpose().fillna('NaC') # Nac = Not a Combination\n",
    "mean_room_prop_prices.to_csv('DataFrames\\\\mean_room_prop_prices.csv')\n",
    "\n",
    "mean_room_prop_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations: \n",
    "* The most expensive room and property combination on average is the entire residence of a Villa at 1406 dollars.\n",
    "* The most cheap room and property combination on average is a shared room in a cottage at 28 dollars. (Both the most expensive and most cheap room/property type combinations are likely influenced by the presence of outliers)\n",
    "* There are a lot of room type and property type combinations that don't exist in the data set! This makes sense - how often do you think shared rooms in campers are listed?\n",
    "\n",
    "In general this table gives us somewhat of an idea of how price varies across room and property types. Now let's take a look at how the price varies with the number of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a scatter plot of the listings by the number of reviews and price\n",
    "number_of_reviews = listings_data['number_of_reviews'].value_counts()\n",
    "number_of_reviews.plot(style = 'o', \n",
    "                       figsize = (12, 8), \n",
    "                       alpha=0.5) # So that we can see density\n",
    "\n",
    "plt.title('Number of Reviews Across Price')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Number of Reviews')\n",
    "\n",
    "plt.savefig('Figures\\\\reviews_across_price.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the listings with the most reviews are within the 40 to 250 dollar price range. This makes sense as the number of reviews can be roughly considered to be the number of times a listing has been booked. We can expect that the most reasonably priced, affordable listings have higher traffic. One problem with this assumption: the number of reviews is likely to be a conservative estimate of the actual number of bookings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, I've outputted the description of the listing with the most reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_data.iloc[353]['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the amenities. First, let's examine what the amenities for a listing look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_data['amenities'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit of a mess. The entry is a string that starts and ends with curly braces and has quotation marks around certain amenities and not others. And if we're looking to compare amenities across listings, we need a way to count the number of amenities for each listing and use a standardized form to see if there are common amenities between listings. We should also take a look at an example entry to ensure that the amenities and counts have been processed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of amenities \n",
    "listings_data['amenities_list']= listings_data['amenities'].apply(lambda x: x[1:-1].replace('\"','').lower().split(','))\n",
    "\n",
    "# Also count the number of amenities\n",
    "listings_data['number_of_amenities'] = listings_data['amenities_list'].apply(len)\n",
    "\n",
    "listings_data.amenities_list[0] # Let's see how this looks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much more workable! It might be a useful exercise to see which amenities are most frequently included among listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the amenities to a list\n",
    "full_list = []\n",
    "for words in listings_data.amenities_list:\n",
    "    full_list += words\n",
    "\n",
    "# Use counter to count the most frequent elements in a list and display the top 20 in a DataFrame\n",
    "amenities_count = Counter(full_list) \n",
    "top20 = amenities_count.most_common()[:21]\n",
    "top20df = pd.DataFrame(top20, columns=['Amenity', 'Frequency'])\n",
    "top20df.to_csv('DataFrames\\\\top_20_amenities.csv', index=False)\n",
    "\n",
    "top20df.set_index('Amenity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not very surprising to see \"Wifi\" as the most common amenity to be included in the listings of this data set. 3158 out of 3211 have it listed as an amenity! Following Wifi is what Airbnb considers \"essentials\", which from the \"hosting help\" portion of their website lists as \"toilet paper, soap, one towel per guest, one pillow per guest, and linens for each guest bed\". It's interesting to me that \"essentials\" doesn't include Wifi, given that Wifi is seemingly more common to include as an amenity!\n",
    "\n",
    "Other common amenities offered in the listings include \"smoke detector\", \"heating\", \"kitchen\", \"hangers\", \"iron\", and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also might be a useful exercise to do the same with the description column to see which words hosts use to describe their listing most frequently. Let's clean up the column first, then take a look at an example entry so that we can ensure that the column has been cleaned properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and make everything lowercase\n",
    "listings_data['description_cleansed'] = listings_data['description'].apply(lambda x: re.sub(r'[^\\w\\s]','',x)).apply(lambda x : x.lower())\n",
    "listings_data['description_cleansed'][0] # Let's see how this looks now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for removing stopwords from a list of words\n",
    "def clean_words(column):\n",
    "    full_list = []\n",
    "    for words in column:\n",
    "        words = words.split()\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "        \n",
    "        full_list += words\n",
    "    \n",
    "    return full_list\n",
    "\n",
    "# Use counter to count the most common words and display the top 20 in a DataFrame\n",
    "words_count = Counter(clean_words(listings_data['description_cleansed'])) \n",
    "top20 = words_count.most_common()[:20]\n",
    "top20df = pd.DataFrame(top20, columns=['Word', 'Frequency'])\n",
    "top20df.set_index('Word', inplace=True)\n",
    "\n",
    "top20df.to_csv('DataFrames\\\\word_freq_desc.csv')\n",
    "top20df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common words used in the descriptions of the listings are \"room\", \"oakland\", \"home\", \"kitchen\", and \"bart\". It's also interesting to note that \"San\" and \"Francisco\" are very common words (within the top 20 most frequent words) in the descriptions of the listings - which may indicate that proximity to San Francisco is a strong selling point for Oakland AirBnB listings in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be an interesting exercise to do this with the \"access\" column to see what words hosts most commonly use to describe how to access their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and make everything lowercase\n",
    "listings_data['access_cleansed'] = listings_data['access'].apply(lambda x: re.sub(r'[^\\w\\s]','',x)).apply(lambda x : x.lower())\n",
    "listings_data['access_cleansed'][0] # Let's see how this looks now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the defined function above to apply the same treatment to the access_cleansed column\n",
    "access_word_list = clean_words(listings_data['access_cleansed'])\n",
    "\n",
    "# Use counter to count the most common words and display the top 20 in a DataFrame\n",
    "words_count = Counter(access_word_list) \n",
    "top20 = words_count.most_common()[:21]\n",
    "top20df = pd.DataFrame(top20, columns=['Word', 'Frequency'])\n",
    "top20df.set_index('Word', inplace=True)\n",
    "\n",
    "# Drop the 'missing' word as it is the filler word for missing entires\n",
    "top20df.drop('missing', inplace=True)\n",
    "top20df.to_csv('DataFrames\\\\word_freq_access.csv')\n",
    "top20df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I understand now that finding the most commonly used words in the description for how to access the property isn't the best way to gain an understanding of how properties are typically accessed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Modeling and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because some features present string values, and regression / other calculations that we can make on the data can only be done on numerical values, we have to prepare and slice the data accordingly for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first extract only the numerical value columns needed for modeling as well as columns that we could make into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_cols = ['id', \n",
    "                 'host_response_time', \n",
    "                 'host_response_rate', \n",
    "                 'host_is_superhost', \n",
    "                 'room_type', \n",
    "                 'property_type',\n",
    "                 'accommodates', \n",
    "                 'bathrooms', \n",
    "                 'beds', \n",
    "                 'bedrooms', \n",
    "                 'price', \n",
    "                 'instant_bookable', \n",
    "                 'cancellation_policy',\n",
    "                 'review_scores_rating',\n",
    "                 'number_of_reviews', \n",
    "                 'number_of_amenities']\n",
    "\n",
    "listings_md = listings_data[modeling_cols]\n",
    "listings_md.set_index('id', inplace=True)\n",
    "listings_md.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still work to be done here. For some of these columns, we need to create sets indicator variables that represent the values within each of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies, and fill missing values with 0\n",
    "listings_md = pd.get_dummies(listings_md)\n",
    "listings_md.fillna(0, inplace=True)\n",
    "\n",
    "# Drop collinear columns (use not being a superhost and not being instantly bookable as references)\n",
    "listings_md.drop(columns=['host_is_superhost_f', 'instant_bookable_f'], axis=1, inplace=True)\n",
    "listings_md.to_csv('DataFrames\\\\listings_processed.csv')\n",
    "\n",
    "listings_md.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a final look at all of the columns themselves as well as the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_md.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_md.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Test Variable (price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_md.price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some basic descriptive stats on price, let's see how it's distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot specifications\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "# Create the distplot\n",
    "sns.distplot(listings_md['price'], fit=norm);\n",
    "\n",
    "# Calculate the mean and std deviation, display them on the plot and print them\n",
    "(mu, sigma) = norm.fit(listings_md['price'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "plt.title('Listing Price Distribution')\n",
    "mpl.rc('font', size=10)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "\n",
    "# Create Normal QQ Plot\n",
    "plt.subplot(2, 1, 2)\n",
    "res = stats.probplot(listings_md['price'], plot=plt)\n",
    "\n",
    "plt.savefig('Figures\\\\listings_price_dist_qqplot.pdf')\n",
    "\n",
    "print(\"Skewness: %f\" % listings_md['price'].skew())\n",
    "print(\"Kurtosis: %f\" % listings_md['price'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that price has substantially high right skew and a very sharp peak in the distribution around the mean as displayed in the histogram. All in all, there is heavy deviation in the distribution of price from the normal distirbution. We'll have to make changes to account for these measurements later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corrmat = listings_md.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "heatmap = sns.heatmap(corrmat, vmax=.8, square=True, cmap=\"YlGnBu\")\n",
    "\n",
    "heatmap.figure.savefig('Figures\\\\corr_matrix.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now very obvious to me that there are high correlations between the number of accommodates, bedrooms, beds, and bathrooms. I don't think that this will be problematic (there aren't that many features, and we aren't using a model that is incredibly sensitive to these types of relationships). Let's take a look at the 8 most correlated features with price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 8 Heatmap\n",
    "k = 8 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'price')['price'].index\n",
    "cm = np.corrcoef(listings_md[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, yticklabels=cols.values, xticklabels=cols.values, cmap=\"YlGnBu\")\n",
    "\n",
    "hm.figure.savefig('Figures\\\\corr_matrix_top8.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a closer look at how the number of accommodates (the most correlated feature) relates to price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "sns.boxplot(x=listings_md['accommodates'], y=listings_md.price)\n",
    "\n",
    "plt.savefig('Figures\\\\#acc_price_boxplot.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, this relationship is relatively linear, and it makes sense. The more accommodates, the more expensive the listing. We do observe the presence of around 10 outliers or so, where some of the highest prices are observed for listings that offer residence for a relatively low number of accommodates ($3000+ for 5 accommodates for ex). We do not observe outliers that may be potentially harmful to our models, as there do not appear to be listings that offer residence for a high number of accommodates that cost very little (which can very much skew estimates in directions that are intuitively incorrect). Removing all outliers is likely to be harmful to our model in the case where there may be outliers in the testing data (when we eventually split the data). It is instead important to keep in mind to make our models robust to the presence of outliers. I'll remove the listings where the prices seem a bit too high (although we've seen that prices can vary wildly across a number of different factors such as property type, room type, etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the indices of the outliers and the listings without a price\n",
    "outliers = listings_md[listings_md['price'] > 750].index\n",
    "missingprice = listings_md[listings_md['price'] == 0].index\n",
    "\n",
    "# Drop them\n",
    "listings_md.drop(outliers, inplace=True)\n",
    "listings_md.drop(missingprice, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.boxplot(x=listings_md['accommodates'], y=listings_md['price'])\n",
    "\n",
    "plt.savefig('Figures\\\\#acc_price_boxplot_fixed.pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_md.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've dropped around 13 observations here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before, our price variable is heavily right-skewed, and has a sharper than normal peak. We can make adjustments to address this and deal with other skewed numerical variables in our data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all prices of 0 (just in case), then log + 1 transform price\n",
    "listings_md.price = np.log1p(listings_md['price'])\n",
    "\n",
    "# Plot specifications\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "# Create the distplot\n",
    "sns.distplot(listings_md['price'], fit=norm);\n",
    "\n",
    "# Calculate the mean and std deviation, display them on the plot and print them\n",
    "(mu, sigma) = norm.fit(listings_md['price'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "plt.title('Listing Price Distribution')\n",
    "mpl.rc('font', size=10)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "\n",
    "# Create Normal QQ Plot\n",
    "plt.subplot(2, 1, 2)\n",
    "res = stats.probplot(listings_md['price'], plot=plt)\n",
    "\n",
    "plt.savefig('Figures\\\\listings_price_dist_qqplot_transformed.pdf')\n",
    "\n",
    "print(\"Skewness: %f\" % listings_md['price'].skew())\n",
    "print(\"Kurtosis: %f\" % listings_md['price'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better! One observation: We can see that the empirical data is a bit steeper in the QQ plot than the theoretical normal distribution. Thus, the empirical data is a bit more dispersed than the normal distribution (this makes sense as there was a very heavy right skew in the pre-transformation data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are other skewed numerical variables in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all non categorical variables\n",
    "numeric_vars = listings_md.dtypes[listings_md.dtypes != 'uint8'].index\n",
    "\n",
    "# Find the skew for all of these variables\n",
    "skewed_vars = listings_md[numeric_vars].apply(lambda x: skew(x))\n",
    "skewed_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For positively skewed variables\n",
    "pos_skew = skewed_vars[skewed_vars > 0.75].dropna()\n",
    "pos_skew_feats = pos_skew.index\n",
    "\n",
    "# Apply a log transformation\n",
    "for feat in pos_skew_feats:\n",
    "    listings_md[feat] = np.log1p(listings_md[feat])\n",
    "\n",
    "# For all negatively skewed variables\n",
    "neg_skew = skewed_vars[skewed_vars < -0.75].dropna()\n",
    "neg_skew_feats = neg_skew.index\n",
    "\n",
    "# Apply a log transformation taking into account the negative skew by subtractive the value from the maximum\n",
    "for feat in neg_skew_feats:\n",
    "    listings_md[feat] = np.log1p(listings_md[feat].max() - listings_md[feat])\n",
    "    \n",
    "# Check skew of these variables again\n",
    "skewed_vars = listings_md[numeric_vars].apply(lambda x: skew(x))\n",
    "skewed_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data one last time before modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_md.to_csv('DataFrames\\\\listings_processed_transformed.csv')\n",
    "listings_md.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use regularized regression models which we can easily implement using the scikit-learn library. First, let's import what we need and split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = listings_md['price']\n",
    "X = listings_md.drop('price', axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll also be helpful to define a function that returns the cross-validation root mean squared error (rmse) so that we can very easily evaluate and compare our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cv(model):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train, Y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with L2 Ridge regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgem = Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization parameter for Ridge regression is alpha, which essentially measures the flexibility of our model. The higher it is, the less prone our model will be able to overfit at the cost of decreased flexibility. Let's take a look at the relationship between rmse and alpha using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50]\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]\n",
    "\n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "cv_ridge.plot(figsize = (10, 8))\n",
    "\n",
    "plt.title('Validation')\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel('rmse')\n",
    "\n",
    "plt.savefig('Figures\\\\L2_rmse_vs_alpha.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a local minimum in the graph above. When alpha is too large, the regularization is too strong and the model cannot capture the complexity in the data. However, if we let the model be too flexible with an incredibly small alpha value, the model begins to overfit. Let's take a look at what the minimum achievable rmse value by manipulating alpha is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ridge.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the Lasso model now. Instead of manually manipulating alphas and testing, we can use the built in Lasso CV to ascertain the optimal value of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassom = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, Y_train)\n",
    "rmse_cv(lassom).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Lasso performs slightly worse. Therefore, we'll use ridge regression in order to predict on the test set. But before we do... \n",
    "\n",
    "One thing to note is that Lasso performs feature reduction for you automatically by setting the coefficients of features that it deems unimportant to 0. Let's take a look at how many were dropped, and which features seem to be the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(lassom.coef_, index= X_train.columns)\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = pd.concat([coef.sort_values().head(6), coef.sort_values().tail(6)])\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "imp_coef.plot(kind = 'barh')\n",
    "plt.title('Coefficients in Lasso Model')\n",
    "\n",
    "plt.savefig('Figures\\\\lasso_feat_importance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important positive feature is `accommodates`, which represents the number of people a listing can accommodate. This makes intuitive sense - the higher the number of people going to be given temporary residence, the higher the price. This feature is followed by a couple of categorical features that denote whether the listing is an entire home or apartment, whether the host responds within a few days, and whether that host is a superhost along with features that represent the number of bathrooms and bedrooms that the listing offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to ridge regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Ridge Regressor with the optimal value of alpha we determined earlier\n",
    "ridgem = Ridge(alpha=5)\n",
    "\n",
    "# Fit on the training set\n",
    "ridgem.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the residuals just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame({'preds':ridgem.predict(X_train), 'true':Y_train})\n",
    "preds['residuals'] = preds['true'] - preds['preds']\n",
    "preds.plot(x = 'preds', y = 'residuals', kind = 'scatter', figsize = (10, 8))\n",
    "\n",
    "plt.savefig('Figures\\\\residuals.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals look fine - there doesn't seem to be any visible trends between them and the predictions, and they don't appear to be too large in magnitude either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict on the test set and see how off we are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgepreds = ridgem.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, ridgepreds))\n",
    "print(\"RMSE: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply `xgboost` here and see if we can get a lower rmse. The basic idea behind xgboost has to do with building weak models, making conclusions about the various feature importance parameters, and then using those conclusions to build a stronger model by capitalizing on misclassication error of previous models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an XGBoost Regressor\n",
    "xg_reg = xgb.XGBRegressor(objective = 'reg:linear', colsample_bytree = 0.3, learning_rate = 0.1, max_depth = 2, n_estimators = 300)\n",
    "\n",
    "# Fit regressor to training set, and make predictions on the test set\n",
    "xg_reg.fit(X_train, Y_train)\n",
    "XGBpreds = xg_reg.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, XGBpreds))\n",
    "print(\"RMSE: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to get a slightly smaller RMSE using the boosted regression on the testing set. Let's take a look at the ridge regression predictions vs. the XGB Regression predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({\"XGB\":XGBpreds, \"Ridge\":ridgepreds})\n",
    "predictions.plot(x = \"XGB\", y = \"Ridge\", kind = \"scatter\", title='Comparison of XGB and Ridge Predictions')\n",
    "\n",
    "plt.savefig('Figures\\\\ridgepreds_vs_xgbpreds.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see relatively good correspondence here, although our predictions are still log transformed, so differences in predictions of outliers is not visually apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can visualize our XGBoost regressor is by examining the importance of each feature column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plot_importance method\n",
    "xgb.plot_importance(xg_reg)\n",
    "\n",
    "# Plot Specifications\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.title('Feature Importance')\n",
    "plt.rc('font', size=12)\n",
    "plt.xlabel('F-Score')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "plt.savefig('Figures\\\\xgb_feat_importance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F-Score produced by this method is a very basic feature importance metric that counts the number of times a feature is split on (in the individual trees) while boosting. We can see that, contrary to feature importances ascertained in our Lasso model, the number of amenities is the most important feature according to our XGBregressor model. The number of reviews also holds a very high value of importance along with the review scores rating and host response rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a more robust XGB model we should do a k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training DMatrix\n",
    "dtrain = xgb.DMatrix(X, label = Y)\n",
    "\n",
    "# Initialize parameters\n",
    "params = {'max_depth':2, 'eta':0.1}\n",
    "model = xgb.cv(params, dtrain, num_boost_round=500, early_stopping_rounds=100, as_pandas=True)\n",
    "\n",
    "# Plot the trends in testing and training rmse's relative to iteration\n",
    "model.loc[30:, ['test-rmse-mean', 'train-rmse-mean']].plot(figsize = (10,8))\n",
    "plt.title('RMSE Trends')\n",
    "plt.xlabel('Boost Round')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "plt.savefig('Figures\\\\cross_val_rmse.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the final boosting round test RMSE below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['test-rmse-mean'].tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis project, I analyzed more of the general trends with regards to of Airbnb listings in the city of Oakland, CA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Descriptive Stats\n",
    "* There are 3211 Airbnb listings in the city of Oakland (in the dataset)\n",
    "* The average price of these listings is \\\\$130.07\n",
    "* There are 2288 unique hosts that have listings in the city of Oakland\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighborhood Information\n",
    "* The neighborhoods in Oakland that contain the most listings are Bushrod (138), Prescott (135), and Longfellow (119)\n",
    "* The neighborhoods with more than 60 listings that have highest average price are Claremont (\\\\$388.78), Upper Rockridge (\\\\$225.00), and Trestle Glen (\\\\$192.06)\n",
    "    * It's difficult to characterize the 3 \"most expensive\" neighbhorhoods as the number of listings per neighborhood is realtively small (around 58 on average), making mean neighborhood listing prices sensitive to the presence of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing Characteristics\n",
    "* Listings tend to be clustered around Bart stations\n",
    "* Listing prices vary to a great extent across different property type and room type combinations (this makes intuitive sense)\n",
    "    * For example, shared Rooms in apartments are \\\\$47.68 on average, whereas entire houses are \\\\$226.73 on average.\n",
    "* The listings with the most reviews are within the \\\\$40 - \\\\$200 price range.\n",
    "* The most common amenities offered by listings are (in order) wifi, \"essentials\" (which include toilet paper, soap, towels, pillows, and sheets), smoke detectors, and heating.\n",
    "* A commonly used term in listings' descriptions is \"San Francisco\" suggesting that proximity to San Francisco is a draw to Oakland Airbnb listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price Prediction\n",
    "* The feature in the dataset that is the most correlated with listings price is the # of accommodates\n",
    "* The most important features in determining listings price according to Lasso Regression are the # of accommodates, whether the listing offers an entire property or not, and the # of bedrooms and bathrooms\n",
    "    * Whether or not the host is a superhost is also an important feature according to the Lasso model\n",
    "* The most important features in determining listings price according to our XGBoost regressor are the # of amenities, the # of accommodates, and the # of the reviews\n",
    "    * The host response rate and the review scores rating are also important features according to this model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
